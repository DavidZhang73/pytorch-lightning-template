# Global Seed # https://lightning.ai/docs/pytorch/stable/common/trainer.html#reproducibility
seed_everything: 3407
# Custom
ignore_warnings: false # Ignore warnings
test_after_fit: false # Apply test after fit finished
git_commit_before_fit: false # Commit before fit
# Trainer Config https://lightning.ai/docs/pytorch/stable/common/trainer.html
trainer:
  # Train, Validate, Test and Predict
  max_epochs: -1
  min_epochs: null
  max_steps: -1
  min_steps: null
  max_time: null

  num_sanity_val_steps: 2
  check_val_every_n_epoch: 1
  val_check_interval: null
  overfit_batches: 0.0

  limit_train_batches: null
  limit_val_batches: null
  limit_test_batches: null
  limit_predict_batches: null

  # Device https://lightning.ai/docs/pytorch/stable/common/trainer.html#accelerator
  accelerator: gpu # "cpu", "gpu", "tpu", "ipu", "auto"
  devices: 1 # "2," for device id 2
  num_nodes: 1 # https://lightning.ai/docs/pytorch/stable/common/trainer.html#num-nodes

  # Distributed
  strategy: auto # https://lightning.ai/docs/pytorch/stable/common/trainer.html#strategy
  sync_batchnorm: false # https://lightning.ai/docs/pytorch/stable/common/trainer.html#sync-batchnorm
  use_distributed_sampler: true # https://lightning.ai/docs/pytorch/stable/common/trainer.html#lightning.pytorch.trainer.Trainer.params.use_distributed_sampler

  # Logger https://lightning.ai/docs/pytorch/latest/visualize/loggers.html
  # https://lightning.ai/docs/pytorch/latest/api_references.html#loggers
  logger:
    # https://lightning.ai/docs/pytorch/latest/api/lightning.pytorch.loggers.wandb.html#module-lightning.pytorch.loggers.wandb
    class_path: WandbLogger
    init_args:
      save_dir: logs
      project: mnist
      log_model: false
  log_every_n_steps: 50

  # Callbacks https://lightning.ai/docs/pytorch/latest/extensions/callbacks.html
  callbacks:
    # https://lightning.ai/docs/pytorch/latest/api/lightning.pytorch.callbacks.ModelCheckpoint.html#lightning.pytorch.callbacks.ModelCheckpoint
    - class_path: ModelCheckpoint
      init_args:
        filename: epoch={epoch:02d}-val_acc={val/acc:.4f}
        monitor: val/acc
        verbose: true
        save_last: true
        save_top_k: 2
        mode: max
        auto_insert_metric_name: false
    # https://lightning.ai/docs/pytorch/latest/api/lightning.pytorch.callbacks.EarlyStopping.html#lightning.pytorch.callbacks.EarlyStopping
    - class_path: EarlyStopping
      init_args:
        monitor: val/acc
        min_delta: 0.01
        patience: 5
        verbose: true
        mode: max
        strict: true
    # https://lightning.ai/docs/pytorch/latest/api/lightning.pytorch.callbacks.LearningRateMonitor.html#lightning.pytorch.callbacks.LearningRateMonitor
    - class_path: LearningRateMonitor
      init_args:
        logging_interval: epoch

  # Gradient Clipping https://lightning.ai/docs/pytorch/stable/common/trainer.html#gradient-clip-val
  gradient_clip_val: null
  gradient_clip_algorithm: null

  # Gradient Accumulation https://lightning.ai/docs/pytorch/stable/common/trainer.html#accumulate-grad-batches
  accumulate_grad_batches: 1

  # Precision https://lightning.ai/docs/pytorch/stable/common/trainer.html#precision
  precision: 32-true

  # Plugins https://lightning.ai/docs/pytorch/stable/common/trainer.html#plugins
  plugins: null

  # Debug
  fast_dev_run: false # https://lightning.ai/docs/pytorch/stable/common/trainer.html#fast-dev-run
  profiler: null # https://lightning.ai/docs/pytorch/stable/api_references.html#profiler
  barebones: false
  detect_anomaly: false
  reload_dataloaders_every_n_epochs: 0

  # Misc
  inference_mode: true
  default_root_dir: null
  benchmark: null
  deterministic: false
  enable_progress_bar: true
  enable_checkpointing: true
  enable_model_summary: true
